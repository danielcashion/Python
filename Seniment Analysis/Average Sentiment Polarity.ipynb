{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps within this notebook pertain to cross-referencing holdings data with \n",
    "the McLaughlin- library for contextual meaning to arrive at a sentiment for an earnings release\n",
    "\n",
    "One of the results is \"Average Sentiment Polarity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from textblob import TextBlob\n",
    "import quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next Configure Quandl API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quandl.ApiConfig.api_key = 'H4SeCU3EqMUafd1mTNyK' # This is for cashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sec_ten_q_urls(ticker: str, amount=4):\n",
    "    \"\"\"\n",
    "    :param ticker: A stock ticker\n",
    "    :param amount: How many urls the method retrieves\n",
    "    :return: the urls for the past amount (default 4) ticker 10-Q report Filing Details.\n",
    "    \"\"\"\n",
    "    r = requests.get(\n",
    "        \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type=10-Q&dateb=&owner=exclude&count=\"\n",
    "        \"{}\".format(ticker, str(amount)))\n",
    "    soup = BeautifulSoup(r.text)\n",
    "\n",
    "    ten_q_table = soup.find_all(\"table\", class_=\"tableFile2\")[0]\n",
    "    # the first row is the header: does not have data we want, so get all rows after index 1\n",
    "    ten_q_table_rows = ten_q_table.find_all(\"tr\")[1:]\n",
    "    urls = []\n",
    "    for i in ten_q_table_rows:\n",
    "        urls.append(\"https://www.sec.gov\" + i.find_all(\"td\")[1].a.get(\"href\"))\n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_soups(urls: list):\n",
    "    \"\"\"\n",
    "    Creates a list of Beautiful Soup objects (one created for the text of each url in urls, a list of URLs)\n",
    "    \"\"\"\n",
    "    soups = []\n",
    "    for i in urls:\n",
    "        td_request = requests.get(i)\n",
    "        soups.append(BeautifulSoup(td_request.text, \"html.parser\"))\n",
    "    return soups\n",
    "\n",
    "\n",
    "def get_date(ten_q_filing_detail_soup):\n",
    "    \"\"\"\n",
    "    Given a Beautiful Soup object representing a Filing Detail Page, return on what data the 10-Q was filed.\n",
    "    \"\"\"\n",
    "    return ten_q_filing_detail_soup.find_all(\"div\", class_=\"info\")[0].get_text()\n",
    "\n",
    "\n",
    "def find_overview_str(text):\n",
    "    \"\"\"\n",
    "    Returns the index of the first occurrence of 'Overview' after 'MANAGEMENT’S' in text\n",
    "    \"\"\"\n",
    "    ITEM_2_index = text.index('MANAGEMENT’S')\n",
    "    return text[text[ITEM_2_index:].index('Overview') + ITEM_2_index:]\n",
    "\n",
    "\n",
    "def get_text_from_filing_detail_soup(ten_q_filing_detail_soup: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Returns the text of the 10-Q report from its Filing Detail page\n",
    "    \"\"\"\n",
    "    # the <tr> with the html document we want.\n",
    "    table_row = ten_q_filing_detail_soup.table.find_all('tr')[1]\n",
    "    # request the quarterly earnings report.\n",
    "    req = requests.get(\"https://www.sec.gov/{}\".format(str(table_row.find_all('td')[2].a.get('href'))))\n",
    "\n",
    "    file_soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    return file_soup.get_text()\n",
    "\n",
    "\n",
    "def get_text_sentiment(text: str):\n",
    "    \"\"\"\n",
    "    Given a piece of text, returns the average sentiment polarity of its sentences\n",
    "    \"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    count = 0\n",
    "    total = 0.0\n",
    "    for sentence in blob.sentences:\n",
    "        total += sentence.sentiment.polarity\n",
    "        count += 1\n",
    "    return total / count\n",
    "\n",
    "\n",
    "def get_nearest_date(date, stock_df):\n",
    "    \"\"\"\n",
    "    Given a date in the format YYYY-MM-DD, returns the date closest to data in stock_df larger than date\n",
    "    \"\"\"\n",
    "    formatted_date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    while datetime.strftime(formatted_date, \"%Y-%m-%d\") not in stock_df.index:\n",
    "        formatted_date += timedelta(days=1)\n",
    "    return datetime.strftime(formatted_date, \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def write_stock_history_json(stock_dataframe, ticker):\n",
    "    \"\"\"\n",
    "    Given a data frame representing a stock's history, writes that DF to a file named ticker+\"_history\".json\n",
    "    :param stock_dataframe:\n",
    "    :param ticker:\n",
    "    \"\"\"\n",
    "    array_of_dicts = stock_dataframe.reset_index().to_dict('records')\n",
    "\n",
    "    for i in range(len(array_of_dicts)):\n",
    "        date = str(array_of_dicts[i][\"date\"])\n",
    "        date = date[:date.index(\" \")]\n",
    "        array_of_dicts[i][\"date\"] = date\n",
    "    with open('{}_history.json'.format(ticker), 'w') as outfile:\n",
    "        json.dump(array_of_dicts, outfile)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function does all of the real work: writing to json, getting historical stock info data frame.\n",
    "### Uses other functions as auxiliaries\n",
    "### returns a tuple of the historical stock info data frame and the earnings day reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_earnings_day_reports(ticker):\n",
    "    ten_q_urls = get_sec_ten_q_urls(ticker)\n",
    "    ten_q_soups = get_soups(ten_q_urls)\n",
    "\n",
    "    earnings_day_reports = []\n",
    "\n",
    "    max_date = \"\"\n",
    "    min_date = \"\"\n",
    "    for soup in ten_q_soups:\n",
    "        ten_q_info = {\"date\": get_date(soup)}\n",
    "\n",
    "        if max_date == \"\":\n",
    "            max_date = ten_q_info[\"date\"]\n",
    "        min_date = ten_q_info[\"date\"]\n",
    "\n",
    "        ten_q_info[\"sentiment\"] = get_text_sentiment(get_text_from_filing_detail_soup(soup))\n",
    "\n",
    "        earnings_day_reports.append(ten_q_info)\n",
    "\n",
    "    stock_data = quandl.get_table('WIKI/PRICES', qopts={'columns': ['date', 'open', 'close']}, ticker=[ticker],\n",
    "                                  date={'gte': min_date, 'lte': max_date})\n",
    "    stock_data = stock_data.set_index(\"date\")\n",
    "    for report in earnings_day_reports:\n",
    "        date = report[\"date\"]\n",
    "        date = get_nearest_date(date, stock_data)\n",
    "\n",
    "        report[\"open\"] = float(stock_data.loc[date, \"open\"])\n",
    "        report[\"close\"] = float(stock_data.loc[date, \"close\"])\n",
    "\n",
    "    with open(ticker + '_info.json', 'w') as outfile:\n",
    "        json.dump(earnings_day_reports, outfile)\n",
    "\n",
    "    write_stock_history_json(stock_data, ticker)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
